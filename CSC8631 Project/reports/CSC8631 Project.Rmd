---
title: "CSC8631 Project"
author: "Luke Battle"
date: "Student No: 200725640"
output:
  pdf_document: default
  PDF: default
---

```{r setup, include=FALSE}
#knitr setup
knitr::opts_chunk$set(include = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

```{r include=FALSE}
#load project
library(ProjectTemplate)
load.project()

#run analysis
source("src/first_analysis.R")

```

## Introduction/Business Understanding

In this report, we will use the CRISP-DM method to analyse a data set. This will involve us running through several key steps; *Business Understanding*, *Data Understanding*, *Data Preparation*, *Modelling*, *Evaluation* and *Deployment*.

## Business Understanding 

The first step in performing our CRISP-DM analysis is to gain an understanding of the business motivation for the analysis. Our primary goal in this case is to enhance Newcastle University's online resources. Specifically, for the past several years, Newcastle University has ran a free online course called *Cyber Security: Safety at Home, Online, in life*. Several data sets have been compiled each time the course has been ran, detailing several facets of the user experience throughout the course.  Newcastle University seeks to utilise this data to enhance the quality of material taught outside of the classroom, and ultimately promote learner engagement with the course and Newcastle's wider online resources. To achieve this, different elements of the collected data will be examined over consecutive runs, with the aim of identifying any trends both within and between them that may indicate an area where user engagement could be improved.  


## Data Understanding 

Next, before beginning our analysis it is important to fully assess the data, so that we can make informed decisions about the portions of the data that should constitute the foundations of our analysis. In total, Newcastle University has provided 53 csv files that describe a range of features of user interaction with the MOOC over 7 runs of the course. These runs span from September 2016 to  September 2018. For every run there is a file detailing a different property, with a maximum of 8 per run. All runs have the files "Archetype-survey-reponses", "Enrolments", "Leaving Survey Response", "Question Response", "Step Activity" and "Weekly Sentiment Survey Response". In addition, from run 2 to 7 a file on the "Team Members" is included, and from run 3 to 7 there is also a file on the "Video Stats". However, the data quality within some of these files is quite poor. Using custom function **check_empty()** (note that this function and all custom functions are located in the lib subdirectory in the helpers.r file), we can check the emptiness of these files:

```{r echo=TRUE}
check_empty(dir("data",pattern = "archetype-survey-responses"))

check_empty(dir("data",pattern = "leaving-survey-responses"))

check_empty(dir("data",pattern = "weekly-sentiment-survey-responses"))
```

Therefore, in the files labelled "archetype-survey-responses", "leaving-survey-responses" and "weekly-sentiment-survey-responses" we can observe that there are several that are totally blank. The inconsistency of data in these files means they might not be the best candidates for the focus of the data analytics pipeline, and so will not feature in this analysis. The "enrolments" data set does contain data, however the quality of it is poor. For `r format(round(mean(percentage_of_missing_enrolments),2))`% of the individuals in the data set, the only data available is the detected country of origin and enrollment date. These individuals are missing useful demographic indicators gender, age range, education level and employment status. Due to the poor quality of the data this will not be used in the analysis. Out of the remaining data sets, a quick look at the video-stats and question-response files suggests that they may warrant further examination and could provide a route to find areas of improvement in the MOOC course. 

### Describe Data

The video stats files are included for course runs 3 to 7. We can immediately observe that there are no empty files. Note that, upon loading this file, all video-stats file names have been saved in vector video_files, so we will use this when checking through them all:

```{r echo=TRUE}
check_empty(video_files)
```

Using the video-stats file from run 3 as an example, let's check the dimensions of the files:

```{r echo=TRUE}
dim(video_file_raw)
```

Using custom function **check_dimensions()**, we can check if this is the case for all video-stats files using the following code: 

```{r}
check_dimensions(video_files, dim(video_file_raw))
```

Therefore, each video-stats file has the same dimensions. The example file from the third run of the course has the following variables:

```{r echo=TRUE}
colnames(video_file_raw)

```

Now, using custom function **check_variables()** we can see that every video-stats file has these variables in the same order. 

```{r echo=TRUE}
check_variables(video_files, colnames(video_file_raw))
```

This means that we can easily compile all of these files into a single master file without any data ended up in the incorrect position. We will do this in the *Data Preparation* phase of our analysis, and this will enable us to more easily perform detailed analysis on the video-stats data set as we will not have to loop through all of the files.

To further our understanding of this data. we can now check that each individual in the file is given by the video and step_position that the variables relate to. These are the following:

```{r echo=TRUE}
video_file_raw[,c("title","step_position")]
```

Using custom function **check_rows()**, we can confirm that the title and step_position are the same in each video-stats file.

```{r}
check_rows(video_files,video_file_raw$title,which(colnames(video_file_raw)=="title"))

check_rows(video_files,video_file_raw$step_position,which(colnames(video_file_raw)=="step_position"))
```


An additional aspect of the video stats files that is important to consider for our analysis is whether the videos used in the course are the same for each run. If they are not, we could not fairly compare the efficacy of each video against each other when considering the results across runs. However, by examining the "duration" variable in each file we can see that this is identical for each video in every run, so we can assume for the purposes of this analysis that the videos have not been changed between runs. Note that we can confirm this with the below code, once again using function **check_rows()**:

```{r}
check_rows(video_files,video_file_raw$video_duration,which(colnames(video_file_raw)=="video_duration"))
```


For the question response data set, we can compute the same high level analysis. However, unlike the video stats files this file has been prepared for all 7 runs of the course, and is not in exactly the same format. Each file has `r format(round(ncol(question_file_raw)))` columns, however the rows in each data file is dependent on how many learners took part in the corresponding run, and so varies between files. In total, all 7 files have `r format(round(nrow(question_df)), big.mark = ",")` rows. 

### Data Quality

We can now assess the data quality of each of these data sets using the dlookr package to see if there is any missing data in each file. Starting with the video stats file from the third run of the course, we calculate the following:

```{r echo=TRUE}
diagnose(video_file_raw)
```

The above table shows that there is no missing data for this video stats file, and analysis on the rest of the files for video stats gives the same result. Now, we can apply the same process to the question file from the first run.

```{r echo=TRUE}
diagnose(question_file_raw)
```

The results indicate that all of the data for the "cloze_response" variable is missing, so we can consider removing this variable when we prepare our data files for analysis. Aside from this variable, the remainder of the data is intact. Analysis on this file for the rest of the course runs yields the same result, in that all data for "cloze_response" is absent whilst the rest of the data is of good quality.



## Data Preparation

In order to prepare the data for analysis it would be helpful to compile all of the files for a specific aspect of the user experience, such as video stats, into one file. This will allow us to easily perform analysis to see how the video stats changed over runs. Additionally, as we have assumed that the videos are unchanged between runs, this will give us more data to assess video performance against each other. In the previous section, we verified that the video stats data files have no issues with regards to missing data, and so there is no need to take any further cleaning steps. Additionally, as previously verified, each video file is in the same format, with the variables in the same order. Therefore we can bind each data frame under the preceding one. However, it would be useful for our subsequent analysis for us to include an identifier of what iteration of the course the data corresponds to. This has been added into the data frame as the variable, "Run". 

We now have a a data frame featuring all of the available data for the video stats from runs 3 to 7 of the course. However, this includes 29 variables. It would therefore be useful to refine this data set into the data that we would like to specifically assess and visualise in our subsequent analysis. Before performing a deeper analysis on the video stats data that we have prepared, it would be useful to first motivate this analysis. One way of doing this would be to look for a relationship between the viewing statistics per run and percentage of questions answered correctly in the question response data set. More specifically, does the amount of people who viewed the videos in each course iteration suggest anything about how many questions will be answered correctly? To investigate this, the variables that will be of most use are "Run" and the percentage of learners that viewed a set proportion of the video, ranging from 5% to 100%. We will calculate the mean average over all videos in a run of the course, for each proportion of the video viewed. In order to effectively visualise this, we can then convert this data frame to a long format, as below:

```{r echo=TRUE}
head(video_df_long)
```

For the question response files, we can facilitate easier cross-run analysis by compiling all files into one, and adding an identifier labeling the run that the data pertains to. This can be performed in the same fashion as with the video stats data frame, however we established in the previous section that the variable in the question response data, "cloze response", is empty across all files. We should therefore remove this variable from our final data set to clean it. In order to calculate the percentage of questions answered correctly per run, it would be useful to transform the "correct" variable into a more calculation-friendly format. We will add a new column, "correct_binary", where if a question has been answered correctly this variable will contain a 1. If this question has been answered incorrectly it will contain a 0. This will enable us to easily calculate the percentage of questions answered correctly within each run of the course.

Once we have motivated our further analysis of the video stats, we can begin evaluating the effectiveness of each video in order to seek areas of improvement. Before beginning this analysis, we should prepare the data in the appropriate format. We will use similar fields that were used in our previous data construction, but we will use the field "step_position" instead of "Run" as an identifier for each unique video. "Step_position" is a code that is assigned to each video in the course, so each step_position will correspond to a video title, it simply allows for a more concise way of representing the same data. We can then take a mean average over every run for each video , which will inform us of how much of each video was watched by a percentage of users. To make visualisation of this data set easier, we will then convert to long format to give us the below:

In order to judge more specifically which videos lost their audience the most, we will add a new variable to this data set, "percentage_drop_off", that calculates the difference in the percentage who viewed 5% of the video and the percentage who viewed 95% of the video. Why we have chosen these values will be discussed later in our analysis. Note that this data set is in wide form for ease of calculation of the "percentage_drop_off" variable. Thus, giving us the below data frame:


## Modelling

Now that both data sets are appropriately formatted, we will begin our analysis of the data. First of all, we will investigate if there is a relationship between the percentage of people who watched the videos in the course, and their results in the multiple choice quiz. To examine this, we will calculate the percentage of questions answered correctly for the each run of the course. 

```{r echo=FALSE}
knitr::kable(correct_questions, col.names=c("Run", "% Correct"), align = "c", digits = 2)
```

To compare this to the percentage of people who watched the videos, we will first take an average over each run of the percentage of people that watched the video. To be fully clear in our analysis, we will also split these average out into percentage of the video viewed. This gives the following plot:

```{r echo=FALSE}
run_plot
```

In the above plot, we can observe a steady decline of people watching the video as it progresses up until 95% of the video. A steep decline can then be seen after this with the amount of people who viewed 100% of the video. This intuitively makes sense as many viewers may end the video when there are only a few seconds left when it is obvious that the the important information in the video has been given. This means that the percentage of people that viewed the entire video is not the best metric by which we can judge how many people watched the video. The percentage of users that watched 95% of the video would be a more suitable metric, as they have still effectively watched the entirety of the video just may have not watched the final few seconds. We can now visualise the percentage of users that viewed 95% of all of the videos for each run of the course, and compare this to a plot of how many questions were answered correctly for each course iteration:


```{r echo=FALSE}
grid.arrange(question_plot,run_plot2,nrow = 2)
```

From this plot we can see clearly that in the fourth run of the MOOC course, there was a drop both in the percentage of questions answered correctly and the amount of people who viewed the course videos. Additionally, in the fifth run both the percentage of people who watched 95% of the videos increased along with the percentage of questions answered correctly. In the sixth run, both the video viewership and questions answered correctly stay relatively high. However, in the seventh run the questions answered correctly stays high whilst the percentage of people who fully watched the videos plummets. In the third run, we can also observe that the highest percentage of questions are answered correctly whilst the percentage of users that watched 95% of the videos remained quite low. These observations would suggest that there is a loose relationship between these two, as we see a similar trend specifically from run 4 to 6. Having said this, rather than causing more questions to be answered correctly, a higher percentage of users that viewed the videos may indicate more interested users in that run who may be more interested in the course and thus excel more in the questions on average. 

Now that we have demonstrated there is some relationship between the use of the video and outcome of the multiple choice quiz, we should look further into how the videos might be improved. In order to assess this, we will assess the average percentage of users who viewed the videos for varying lengths of time, for each video in the course. As previously noted, we are assuming that the videos have remained unaltered over runs due to the identical duration, so these average will be taken for each video over every run. Plotting the results of this gives us the following plot:

```{r echo=FALSE}
step_plot
```

As this plot appears cluttered, it would be useful to only focus on a selection of the videos that had the highest drop off in audience as the video progressed. We will therefore filter the above plot to show only the top 5 videos with the highest % drop in audience to produce the below plot:

```{r echo=FALSE}
step_plot_filtered
```

This plot indicates that video with step position 1.5 loses the highest percentage of it's audience. This corresponds to the video called "Privacy online and offline". What is particularly notable with this video, is the steep drop in audience from 5% to 25% of the video. Other videos in the above two plots have a fairly consistent gradient of audience decline as the video progresses. This video displays a significantly higher drop in audience that any other video on the course over all runs from 5% to 95% of the video, as shown in the below column chart:


```{r echo=FALSE}
lost_viewer_plot
```

We can also represent this data explicitly and with the inclusion of the video titles, as below:

```{r echo=FALSE}
knitr::kable(highest_audience_drop, col.names=c("Step Position","Video Title", "% Drop in Learner Audience"), align = "c", digits = 2)
```

Again, this shows that the video with step position 1.5, "Privacy online and offline", loses over 20% of the audience from 5% of the video to 95% of it. Comparatively, the video with step position 2.11, "Exploring vulnerabilities in online payments" has slightly less than a 15% loss in viewers. In contrast to these videos, we can observe that the video with step position 2.17, "The million dollar contactless payment", has the highest viewer retention and loses less than 5% of the audience. The mean average percentage loss is `r format(round(mean(highest_audience_drop$percentage_drop_off),2))`%.

Whilst many factors could contribute to how engaged a learner is with a video, an aspect from the video stats data that we could investigate as a driver of this is the duration of the video. To visualise this, we can create a plot of video duration against the average percentage viewed over the course runs. Moreover, we will plot this for the percentage of people that watched 95% of the video, due to the aforementioned drop in viewers who watched all 100% of it. 

```{r echo=FALSE}
duration_plot
```

The above plot shows a clear negative correlation between the length of the video and the percentage of people who watched 95% of it. Interestingly, this correlation is particularly prevalent in the first 300 seconds. After 300 seconds, the percentage of the retained learner throughout the video increased again, but not in the vicinity of retention shown in the videos with small duration. 


## Evaluation

We initially set out to assess measures that Newcastle University could take in order to improve the efficacy of their online teaching resources, specifically the cyber security MOOC that they run. Thus far in this report, we have assessed the quality and suitability of the data supplied by Newcastle University, prepared data sets for the video stats and question response files and performed several pieces of analysis on them. Our analysis has shown several things. Firstly, we established that there is a slight positive correlation between the percentage of learners who watch the videos and the percentage of questions answered correctly in the course quiz. Our intention was to motivate the subsequent analysis on the video stats data, so that before seeking improvements in the course videos, we should assess the efficacy of the videos as a learning tool. Although our analysis suggests there is a correlation between the two course components, we could attribute this to many things. For instance, it may not be the videos themselves that can impact learning on the course, but poor quiz results like in run 4 could simply reflect learners less engaged with the course, irrespective of whether they watched the video or not.

The second component in our analysis was a comparison of learner engagement between videos. As noted previously, we assumed that the videos remained unchanged in each run of the course, so when comparing videos we can take an average of learning engagement with each video over all course runs. This analysis highlighted several videos that had a particularly low retention of viewers, but of particular note was the video entitled "Privacy online and offline", which had a far lower audience retention rate than any other video, with a high drop off in audience during the first quarter of the video. It is therefore clear that this video could be improved with the goal of enhancing learner engagement. 

The final part of our analysis consisted of an investigation into whether the duration of the videos could be leading to poor audience retention. Our results indicated that shorter videos do tend to increase learner engagement, with far less learners leaving videos early that were less than 100 seconds long. However, as with our first analysis this interpretation is far from conclusive and there are many other potential reasons that learners would be less engaged with some videos than others.

Out of the results above from our analysis, the second part as an identification for an area of improvement in the course is the most significant. There is clearly scope to promote the efficacy of the video "Privacy online and offline" as a tool for learning, either by condensing the material into a shorter video or by another means. Then, the result of the video duration vs audience retention gave the second most compelling result. Although the result was not conclusive, it showed a clear correlation between the length of the videos and the amount of learners that stayed watching them. Where the second analysis identified the area of improvement, the third provided a method that this area could potentially be enhanced, therefore satisfying the business objectives that we initially set. Finally, as the results only slightly agreed for runs 4,5 and 6, the comparison of the questions answered correctly in the quiz and the percentage of learners who watched 95%  of the videos yielded the most unsuccessful results. Even if our comparison was more successful, there are so many factors that could influence learners performance in the quiz that they would still be far from conclusive. 

These analyses could be improved and built upon in several ways. Our first analysis could be taken to a more granular level, and consider comparing specific sections of relevance between the quiz and the videos, i.e. comparing a video to the section of the quiz that it relates to. Additionally, continually updating this analysis as more course runs occur would improve all aspects of analysis, as more data could either disprove or reinforce some of our intepretations with regard to the analysis outcomes.

For the majority of the data preparation, steps have been taken where possible to promote computing efficiency. When empty vectors have been created to store results, their length has been specified so that it is populated efficiently. In particular, the method used in preparing the data frames that we have performed analysis on has been done efficiently. Rather than using rbind() with an empty data frame, we used a list with a specified length to store the data from each file and then the function bind_rows() to compile them. These methods allow us to avoid increasing object size incrementally in our data preparation, which would not scale well if we were to perform the analysis for significantly more files. Having said that, parallel computing has not been used in any data preparation steps, so if this were to be improved it could be by using parallel computing to enhance the efficiency of the data preparation process. 



## Deployment

The analysis performed is fully reproducible, and any new course runs can be added to the "data" sub-directory within the project folder. Loading the project will then facilitate any new data files for video-stats or question-response to be filtered through the analysis and update the appropriate plots. Note that the reproducibility of the project is dependent on the both the files and filenames being in the format they are for this analysis. An an example, in run 3 the video-stats and question-response filenames would be "`r video_files[1]`" and "`r question_files[3]`", respectively. Furthermore, by taking care to avoid code that incrementally grows objects, the steps taking to prepare the data are computationally robust should significantly more data files be ran through the analysis. Once any new data files have been formatted appropriately and filtered through the corresponding graph, they will also appear in an interactive presentation created using shiny. The plots in the presentation are linked to the plots prepared in this analysis, and so any new data will be reflected there. None of this requires any user input and so the process is fully reproducible with minimal effort. The presentation surmises the three key points from the analysis in the comparison of video viewing vs question response, analysis of learner retention per video and the effect of video duration on this.










